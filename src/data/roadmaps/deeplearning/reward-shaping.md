# Reward shaping resource
Reward shaping is a technique used in reinforcement learning to improve the learning process of an agent. It involves modifying the rewards given to an agent in order to provide additional guidance or incentive for the agent to learn more efficiently.

# Learning Source
[What is reward shaping in ML?](https://openreview.net/forum?id=W7HvKO1erY)

[Belief Reward Shaping in Reinforcement Learning](https://ojs.aaai.org/index.php/AAAI/article/view/11741)

[Theory and Application ](https://www.ideals.illinois.edu/items/10802)

[Reference Link](https://crossminds.ai/video/learning-to-utilize-shaping-rewards-a-new-approach-of-reward-shaping-606fe26bf43a7f2f827c0093/)

[What is Data Augmentation? The Complete Guide!](https://www.amygb.ai/blog/what-is-data-augmentation)

